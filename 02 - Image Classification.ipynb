{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 画像の分類\n",
        "\n",
        "*Computer Vision* Cognitive Service には画像を扱うための便利なモデルがあらかじめ組み込まれていますが、多くの場合、独自のモデルを Computer Vision でトレーニングすることが必要になります。たとえば、小売店 Northwind Traders で、精算時にカメラで撮影した画像に基づいて、顧客が購入したい食料品を識別する自動精算システムを作成するシナリオを想定してみます。これを行うには、画像を分類できる分類モデルをトレーニングして、購入する商品を識別できるようにする必要があります。\n",
        "\n",
        "![クリップボードを持って、リンゴ、バナナ、オレンジの絵を分類するロボット](./images/image-classification.jpg)\n",
        "\n",
        "Azureでは、***Custom Vision*** Cognitive Service を使用して、既存の画像に基づいて画像分類モデルをトレーニングできます。画像分類ソリューションの作成には 2 つの手順があります。まず、既存の画像を使用してさまざまなクラスを認識できるように、モデルをトレーニングする必要があります。次に、モデルをトレーニングしたら、アプリケーションで使用できるサービスとしてモデルを発行する必要があります。\n",
        "\n",
        "## Custom Vision リソースを作成する\n",
        "\n",
        "Custom Vision サービスを使用するには、モデルの*トレーニング*に使用する Azure リソースと、アプリケーションで使用できるようにモデルを*発行*するためのリソースが必要です。これらのタスクのリソースは、一般的な **Cognitive Services** リソースであっても、特定の **Custom Vision** リソースであっても構いません。タスクごとに同じ Cognitive Services リソースを使用したり、同じリージョン内の異なるリソースを使用してコストを個別に管理したりすることもできます。\n",
        "\n",
        "次の手順で、新しい **Custom Vision** リソースを作成します。\n",
        "\n",
        "1. ブラウザーの新しいタブで、Azure portal ([https://portal.azure.com](https://portal.azure.com)) を開き、Azure サブスクリプションに関連付けられている Microsoft アカウントでサインインします。\n",
        "2. 「**&#65291;リソースの作成**」 ボタンをクリックし、*Custom Vision* を検索して、以下の設定で **Custom Vision** リソースを作成します。\n",
        "    - **オプションを作成**: 両方\n",
        "    - **サブスクリプション**: *使用する Azure サブスクリプション*\n",
        "    - **リソース グループ**: *一意の名前のリソース グループを選択または作成します*\n",
        "    - **名前**: *一意の名前を入力*\n",
        "    - **トレーニング場所**: *利用可能な任意のリージョンを選択します*\n",
        "    - **トレーニングの価格レベル**: F0\n",
        "    - **予測場所**: *トレーニングのリソースと同じリージョン*\n",
        "    - **予測の価格レベル**: F0\n",
        "\n",
        "    > **注意**: サブスクリプションに F0 Custom Vision サービスが既にある場合は、「**S0**」 を選択します。\n",
        "\n",
        "3. リソースが作成されるのを待ち、2 つの Custom Vision リソースがプロビジョニングされたことを確認してください。1 つはトレーニング用で、もう 1 つは予測用です。リソースを作成したリソース グループに移動すると、これらのリソースを表示できます。\n",
        "\n",
        "## Custom Vision プロジェクトを作成する\n",
        "\n",
        "物体検出モデルをトレーニングするには、トレーニング リソースに基づいて Custom Vision プロジェクトを作成する必要があります。Custom Vision ポータルを使用します。\n",
        "\n",
        "1. https://aka.ms/fruit-images からトレーニング画像をダウンロードして抽出します。**注:** 一時的な回避策として、トレーニングイメージにアクセスできない場合は、https://www.github.com にアクセスしてから、https://aka.ms/fruit-images にアクセスしてください。  \n",
        "2. ブラウザーの別のタブで、[https://customvision.ai](https://customvision.ai) の Custom Vision ポータルを開きます。プロンプトが表示されたら、Azure サブスクリプションに関連付けられている Microsoft アカウントでサインインし、利用規約に同意します。\n",
        "3. Custom Vision ポータルから、次の設定で新しいプロジェクトを作成します。\n",
        "    - **名前**: Grocery Checkout\n",
        "    - **説明**: 食料品の画像分類\n",
        "    - **リソース**: *事前に作成した Custom Vision リソース*\n",
        "    - **プロジェクトのタイプ**: Classification\n",
        "    - **分類のタイプ**: マルチクラス (画像ごとに 1 つのタグ)\n",
        "    - **ドメイン**: Food\n",
        "4. 「**\\「+\\」 画像を追加**」 をクリックして、事前に抽出した**りんご**フォルダー内のすべてのファイルを選択します。次のように *りんご* タグを指定して、これらの画像ファイルをアップロードします。\n",
        "\n",
        "![りんごタグが付いたりんごをアップロード](./images/upload_apples.jpg)\n",
        "   \n",
        "5. 前の手順を繰り返して、*バナナ*というタグが付いた**バナナ**フォルダー内の画像と、*みかん*というタグが付いた**みかん**フォルダー内の画像をアップロードします。\n",
        "6. Custom Vision プロジェクトにアップロードした画像を確認します。次のように、各クラスの画像がそれぞれ 15 枚あるはずです。\n",
        "\n",
        "![くだもののタグが付いた画像。15 個のリンゴ、15 個のバナナ、15 個のみかん](./images/fruit.jpg)\n",
        "    \n",
        "7. Custom Vision プロジェクトで画像の上にある 「**トレーニング**」 をクリックして、タグ付けされた画像を使用して分類モデルをトレーニングします。「**クイック トレーニング**」 オプションを選択し、トレーニングの反復が完了するのを待ちます。これには 1 分ほどかかる場合があります。\n",
        "8. モデルの反復トレーニングが修了したら、*正確性*、*再現性*、*AP* などのパフォーマンス メトリックを確認します。これらは分類モデルの予測精度の指標であり、すべて高いはずです。\n",
        "\n",
        "## モデルをテストする\n",
        "\n",
        "アプリケーションで使用するモデルの反復は、発行する前にテストする必要があります。\n",
        "\n",
        "1. パフォーマンス メトリックの上にある 「**クイック テスト**」 をクリックします。\n",
        "2. 「**画像の URL**」 ボックスに`https://aka.ms/apple-image`と入力し、「&#10132;」 をクリックします。\n",
        "3. モデルが返す予測を表示します。次のように、*りんご*の確率スコアが最も高くなります。\n",
        "\n",
        "![りんごのクラス予測を含む画像](./images/test-apple.jpg)\n",
        "\n",
        "4. 「**クイック テスト**」 ウィンドウを閉じます。\n",
        "\n",
        "## 画像分類モデルを発行して利用する\n",
        "\n",
        "トレーニングしたモデルを発行してクライアント アプリケーションから使用する準備が整いました。\n",
        "\n",
        "9. 「**&#128504; 発行**」 をクリックして、トレーニングしたモデルを次の設定で発行します。\n",
        "    - **モデル名**: groceries\n",
        "    - **予測リソース**: *事前に作成した予測リソース*\n",
        "\n",
        "### (!)確認 \n",
        "同じモデル名「**groceries**」を使用しましたか?   \n",
        "\n",
        "10. 発行したら 「**パフォーマンス**」 ページの右上にある 「*設定*」 (&#9881;) アイコンをクリックして、プロジェクトの設定を表示します。左側の 「**全般**」 で、**プロジェクト ID** をコピーします。下にスクロールして、以下のステップ 13 のコード セルに貼り付けます (**YOUR_PROJECT_ID** と置き換える)。\n",
        "\n",
        "![プロジェクト設定のプロジェクト ID](./images/cv_project_settings.jpg)\n",
        "\n",
        "> _**注**: この演習の最初に **Custom Vision** リソースを作成する代わりに **Cognitive Services** リソースを使用した場合は、プロジェクト設定の右側からキーとエンドポイントをコピーして、下のコード セルに貼り付けて実行すると、結果を確認できます。それ以外の場合は、以下の手順を続行して、Custom Vision 予測リソースのキーとエンドポイントを取得してください。\n",
        "\n",
        "11. 「**プロジェクト設定**」 ページの左上にある 「*プロジェクト ギャラリー*」 (&#128065;) アイコンをクリックして、プロジェクトが一覧表示されている Custom Vision ポータルのホームページに戻ります。\n",
        "\n",
        "12. Custom Vision ポータルのホームページの右上にある 「*設定*」 (&#9881;) アイコンをクリックして、Custom Vision サービスの設定を表示します。「**リソース**」 で**予測**リソース (トレーニング リソース<u>ではありません</u>) を展開し、その 「**キー**」 と 「**エンドポイント**」 の値をコピーして、以下の手順 13 のコード セルに貼り付けます (**YOUR_KEY**、**YOUR_ENDPOINT** とそれぞれ置き換える)。\n",
        "\n",
        "### (!)確認 \n",
        "**Custom Vision** リソースを使用している場合、**予測**リソースを使用しましたか (トレーニング リソースでは<u>ありません</u>)?\n",
        "\n",
        "![Custom Vision 設定の予測リソース キーとエンドポイント](./images/cv_settings.jpg)\n",
        "\n",
        "13. セルの左側にある 「**セルの実行**」 (&#9655;) ボタンをクリックして以下のセルのコードを実行し、変数にプロジェクト ID、キー、およびエンドポイント値を設定します。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1599691949340
        }
      },
      "outputs": [],
      "source": [
        "project_id = 'YOUR_PROJECT_ID'\n",
        "cv_key = 'YOUR_KEY'\n",
        "cv_endpoint = 'YOUR_ENDPOINT'\n",
        "\n",
        "model_name = 'groceries' # this must match the model name you set when publishing your model iteration (it's case-sensitive)!\n",
        "print('Ready to predict using model {} in project {}'.format(model_name, project_id))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "これで、キーとエンドポイントを Custom Vision クライアントで使用して、Custom Vision 分類モデルに接続できます。\n",
        "\n",
        "次のコード セルを実行し、発行したモデルを使用して選択したテスト画像を分類します。\n",
        "\n",
        "> **注**: コードの詳細についてはあまり気にしないでください。Computer Vision SDK for Python を使用して、/data/image-classification/test-fruit フォルダーにある各画像のクラス予測を取得します。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1599692327514
        }
      },
      "outputs": [],
      "source": [
        "from azure.cognitiveservices.vision.customvision.prediction import CustomVisionPredictionClient\n",
        "from msrest.authentication import ApiKeyCredentials\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "import os\n",
        "%matplotlib inline\n",
        "\n",
        "# Get the test images from the data/vision/test folder\n",
        "test_folder = os.path.join('data', 'image-classification', 'test-fruit')\n",
        "test_images = os.listdir(test_folder)\n",
        "\n",
        "# Create an instance of the prediction service\n",
        "credentials = ApiKeyCredentials(in_headers={\"Prediction-key\": cv_key})\n",
        "custom_vision_client = CustomVisionPredictionClient(endpoint=cv_endpoint, credentials=credentials)\n",
        "\n",
        "# Create a figure to display the results\n",
        "fig = plt.figure(figsize=(16, 8))\n",
        "\n",
        "# Get the images and show the predicted classes for each one\n",
        "print('Classifying images in {} ...'.format(test_folder))\n",
        "for i in range(len(test_images)):\n",
        "    # Open the image, and use the custom vision model to classify it\n",
        "    image_contents = open(os.path.join(test_folder, test_images[i]), \"rb\")\n",
        "    classification = custom_vision_client.classify_image(project_id, model_name, image_contents.read())\n",
        "    # The results include a prediction for each tag, in descending order of probability - get the first one\n",
        "    prediction = classification.predictions[0].tag_name\n",
        "    # Display the image with its predicted class\n",
        "    img = Image.open(os.path.join(test_folder, test_images[i]))\n",
        "    a=fig.add_subplot(len(test_images)/3, 3,i+1)\n",
        "    a.axis('off')\n",
        "    imgplot = plt.imshow(img)\n",
        "    a.set_title(prediction)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "うまくいけば、画像分類モデルが画像の中の食品を正しく識別しているでしょう。\n",
        "\n",
        "## 詳細情報\n",
        "\n",
        "Custom Vision サービスには、この演習で確認した機能の他にも多くの機能があります。たとえば、Custom Vision Services を使用して*物体検出*モデルを作成することもできます。これによって、画像に写っているオブジェクトを分類するだけでなく、画像の中の物体の場所を示す*境界ボックス*を識別できます。\n",
        "\n",
        "Computer Vision Cognitive Services の詳細については、「[Computer Vision ドキュメント](https://docs.microsoft.com/azure/cognitive-services/custom-vision-service/home)」を参照してください。"
      ]
    }
  ],
  "metadata": {
    "kernel_info": {
      "name": "python3-azureml"
    },
    "kernelspec": {
      "display_name": "Python 3.6 - AzureML",
      "language": "python",
      "name": "python3-azureml"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
