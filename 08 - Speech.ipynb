{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 会話\r\n",
        "\r\n",
        "話しかけると、話し言葉で応答してくれるような人工知能 (AI) システムとのコミュニケーションの実現に向けて、期待が高まっています。\r\n",
        "\r\n",
        "![しゃべるロボット](./images/speech.jpg)\r\n",
        "\r\n",
        "*音声認識* (話し言葉を解釈する AI システム) と*音声合成* (音声応答を生成する AI システム) は、会話を実現する AI ソリューションの重要なコンポーネントです。\r\n",
        "\r\n",
        "## Cognitive Services リソースを作成する\r\n",
        "\r\n",
        "**Speech** Cognitive Service を使用すると、聞こえてくる言葉を解釈して、音声で応答できるソフトウェアを構築できます。このサービスを使用すると、話し言葉をテキストに変換したり、その逆に変換したりすることが簡単に行えます。\r\n",
        "\r\n",
        "まだリソースを作成していない場合は、次の手順で Azure サブスクリプションに **Cognitive Services** リソースを作成します。\r\n",
        "\r\n",
        "> **注**: Cognitive Services リソースが既にある場合は、Azure portal で**クイック スタート**ページを開き、キーとエンドポイントを以下のセルにコピーするだけで作成できます。それ以外の場合は、以下の手順に従って作成してください。\r\n",
        "\r\n",
        "1. ブラウザーの新しいタブで Azure portal (https://portal.azure.com) を開き、Microsoft アカウントでサインインします。\r\n",
        "2. 「**&#65291;リソースの作成**」 ボタンをクリックし、*Cognitive Services* を検索して、以下の設定で **Cognitive Services** リソースを作成します。\r\n",
        "    - **サブスクリプション**: *使用する Azure サブスクリプション*\r\n",
        "    - **リソース グループ**: *一意の名前のリソース グループを選択または作成します*\r\n",
        "    - **リージョン**: *利用可能な任意のリージョンを選択します*。\r\n",
        "    - **名前**: *一意の名前を入力します*。\r\n",
        "    - **価格レベル**: S0\r\n",
        "    - **注意事項を読み理解しました**: 選択されています。\r\n",
        "3. デプロイが完了するまで待ちます。そのあと Cognitive Services リソースに移動し、「**概要**」 ページでリンクをクリックしてサービスのキーを管理します。クライアント アプリケーションから Cognitive Services リソースに接続するには、キーと場所が必要です。\r\n",
        "\r\n",
        "### Cognitive Services リソースのキーと場所を取得する\r\n",
        "\r\n",
        "Cognitive Services リソースを使用するには、クライアント アプリケーションに認証キーと場所が必要です。\r\n",
        "\r\n",
        "1. Azure portalで、Cognitive Services リソースの 「**キーとエンドポイント**」 ページからリソースの 「**キー 1**」 の値をコピーし、以下のコードに貼り付けます (**YOUR_COG_KEY** と置き換える)。\r\n",
        "2. リソースの**場所**をコピーして以下のコードに貼り付けます (**YOUR_COG_LOCATION** を置き換える)。\r\n",
        ">**注**: 「**キーとエンドポイント**」 ページにとどまり、このページから**場所**をコピーします (例: _westus_)。「場所」 フィールドの単語の間には空白を入れ _ないで_ ください。 \r\n",
        "3. セルの左側にある 「**セルの実行**」 (&#9655;) ボタンをクリックして、以下のコードを実行します。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1599695240794
        }
      },
      "outputs": [],
      "source": [
        "cog_key = 'YOUR_COG_KEY'\n",
        "cog_location = 'YOUR_COG_LOCATION'\n",
        "\n",
        "print('Ready to use cognitive services in {} using key {}'.format(cog_location, cog_key))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 音声認識\r\n",
        "\r\n",
        "「明かりをつけて」や「明かりを消して」などの音声指示に対応できるホーム オートメーション システムを構築するとします。アプリケーションは、音声ベースの入力 (話された指示) を受け取り、それをテキストに書き起こして認識し、構文解析と意味を分析できる必要があります。\r\n",
        "\r\n",
        "これで、音声を書き写す準備が整いました。入力は、**マイク**または**音声ファイル**から行うことができます。 \r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 音声ファイルを使用した音声認識\r\n",
        "\r\n",
        "以下のセルを実行し、**音声ファイル**を使用して Speech Recognition サービスが稼働していることを確認します。 \r\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "from playsound import playsound\n",
        "from azure.cognitiveservices.speech import SpeechConfig, SpeechRecognizer, AudioConfig\n",
        "\n",
        "# Get spoken command from audio file\r\n",
        "file_name = 'light-on.wav'\n",
        "audio_file = os.path.join('data', 'speech', file_name)\n",
        "\n",
        "# Configure speech recognizer\r\n",
        "speech_config = SpeechConfig(cog_key, cog_location)\n",
        "speech_config.speech_synthesis_voice_name = 'en-US-ChristopherNeural'\n",
        "audio_config = AudioConfig(filename=audio_file) # Use file instead of default (microphone)\n",
        "speech_recognizer = SpeechRecognizer(speech_config, audio_config)\n",
        "\n",
        "# Use a one-time, synchronous call to transcribe the speech\r\n",
        "speech = speech_recognizer.recognize_once()\n",
        "\n",
        "# Play the original audio file\r\n",
        "playsound(audio_file)\n",
        "\n",
        "# Show transcribed text from audio file\r\n",
        "print(speech.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 音声合成\r\n",
        "\r\n",
        "これで、Speech サービスを使用して音声をテキストに変換する方法を確認できました。ただし、逆の変換はまだです。テキストを音声に変換するにはどうすればよいでしょうか?\r\n",
        "\r\n",
        "ホーム オートメーション システムが明かりをつける指示を解釈したとします。適切な応答とは、指示されたことを口頭で確認したり、指示されたタスクを実際に実行したりすることでしょう。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1599695261170
        }
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "from azure.cognitiveservices.speech import SpeechConfig, SpeechSynthesizer, AudioConfig\n",
        "%matplotlib inline\n",
        "\n",
        "# Get text to be spoken\r\n",
        "response_text = 'Turning the light on.'\n",
        "\n",
        "# Configure speech synthesis\r\n",
        "speech_config = SpeechConfig(cog_key, cog_location)\n",
        "speech_config.speech_synthesis_voice_name = 'en-US-ChristopherNeural'\n",
        "speech_synthesizer = SpeechSynthesizer(speech_config)\n",
        "\n",
        "# Transcribe text into speech\r\n",
        "result = speech_synthesizer.speak_text(response_text)\n",
        "\n",
        "# Display an appropriate image \r\n",
        "file_name = response_text.lower() + \"jpg\"\n",
        "img = Image.open(os.path.join(\"data\", \"speech\", file_name))\n",
        "plt.axis('off')\n",
        "plt. imshow(img)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**response_text** 変数を「*明かりを消して。*」に変更してみてください。最後に句点も追加します。そしてセルをもう一度実行して結果を聞きます。\r\n",
        "\r\n",
        "## 詳細情報\r\n",
        "\r\n",
        "このノートブックでは、音声認識サービスを使用する簡単な例を確認しました。詳細については、Speech サービス ドキュメントの「[speech-to-text](https://docs.microsoft.com/azure/cognitive-services/speech-service/index-speech-to-text)」と「[text-to-speech](https://docs.microsoft.com/azure/cognitive-services/speech-service/index-text-to-speech)」を参照してください。"
      ]
    }
  ],
  "metadata": {
    "kernel_info": {
      "name": "python3-azureml"
    },
    "kernelspec": {
      "display_name": "Python 3.8.5 32-bit",
      "metadata": {
        "interpreter": {
          "hash": "177429bd1865e7f7a0dbecbac90518c0d9641b1102b2e6c0df4b82dc948b5cb2"
        }
      },
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.0"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}