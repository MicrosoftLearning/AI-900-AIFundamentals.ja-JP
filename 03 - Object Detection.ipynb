{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 物体検出\r\n",
        "\r\n",
        "*物体検出*は Computer Vision の機能であり、機械学習モデルをトレーニングすることにより、画像の中にある個々のオブジェクトのインスタンスを分類して、その場所を示す*境界ボックス*を示すことができます。「この画像は何ですか?」という質問にモデルが答える*画像分類*から、「この画像に写っている物体は何ですか、それはどこにありますか?」という質問にモデルが答えるソリューション構築への発展と考えることができます。\r\n",
        "\r\n",
        "![くだものを識別しているロボット](./images/object-detection.jpg)\r\n",
        "\r\n",
        "たとえば、食料品店では、精算レジのベルト コンベヤーの上に並べられた商品を 1 つずつ読み取らなくても、物体検出モデルを使用した自動精算システムを導入すると、カメラでベルト コンベヤーをスキャンして、それらの商品をまとめて識別することができます。\r\n",
        "\r\n",
        "Microsoft Azure の **Custom Vision** Cognitive Service はクラウドベースのソリューションであり、カスタマイズした物体検出モデルを作成して発行することができます。\r\n",
        "\r\n",
        "## Custom Vision リソースを作成する\r\n",
        "\r\n",
        "Custom Vision サービスを使用するには、モデルのトレーニングに使用する Azure リソースと、アプリケーションで使用できるようにモデルを発行するためのリソースが必要です。タスクごとに同じリソースを使用することも、両方のリソースが同じリージョンで作成されていれば、それぞれに異なるリソースを使用してコストを個別に管理することもできます。これらのタスクのリソースは、一般的な **Cognitive Services** リソースであっても、特定の **Custom Vision** リソースであっても構いません。次の手順で、新しい **Custom Vision** リソースを作成します。既存のリソースがある場合は、それを使用しても構いません。\r\n",
        "\r\n",
        "1. ブラウザーの新しいタブで、Azure portal ([https://portal.azure.com](https://portal.azure.com)) を開き、Azure サブスクリプションに関連付けられている Microsoft アカウントでサインインします。\r\n",
        "2. 「**&#65291;リソースの作成**」 ボタンをクリックし、*Custom Vision* を検索して、以下の設定で **Custom Vision** リソースを作成します。\r\n",
        "    - **オプションを作成**: 両方\r\n",
        "    - **サブスクリプション**: *使用する Azure サブスクリプション*\r\n",
        "    - **リソース グループ**: *一意の名前のリソース グループを選択または作成します*\r\n",
        "    - **名前**: *一意の名前を入力*\r\n",
        "    - **トレーニング場所**: *利用可能な任意のリージョンを選択します*\r\n",
        "    - **トレーニングの価格レベル**: F0\r\n",
        "    - **予測場所**: *トレーニング場所と同じ*\r\n",
        "    - **予測の価格レベル**: F0\r\n",
        "\r\n",
        "    > **注意**: サブスクリプションに F0 Custom Vision サービスが既にある場合は、「**S0**」 を選択します。\r\n",
        "\r\n",
        "3. リソースが作成されるまで待ちます。\r\n",
        "\r\n",
        "## Custom Vision プロジェクトを作成する\r\n",
        "\r\n",
        "物体検出モデルをトレーニングするには、トレーニング リソースに基づいて Custom Vision プロジェクトを作成する必要があります。Custom Vision ポータルを使用します。\r\n",
        "\r\n",
        "1. ブラウザーの新しいタブで、[https://customvision.ai](https://customvision.ai) の Custom Vision ポータルを開き、Azure サブスクリプションに関連付けられている Microsoft アカウントでサインインします。\r\n",
        "2. 次の設定で新しいプロジェクトを作成します。\r\n",
        "    - **名前**: 食料品の検出\r\n",
        "    - **説明**: 食料品の物体検出\r\n",
        "    - **リソース**: *事前に作成した Custom Vision リソース*\r\n",
        "    - **プロジェクトのタイプ**: 物体検出\r\n",
        "    - **ドメイン**: 全般\r\n",
        "3. プロジェクトが作成され、ブラウザーが開くのを待ちます。\r\n",
        "\r\n",
        "## 画像を追加してタグ付けする\r\n",
        "\r\n",
        "物体検出モデルをトレーニングするには、検出モデルで識別したい分類を含む画像をアップロードし、各オブジェクトのインスタンスの境界ボックスを示すようにタグを付ける必要があります。\r\n",
        "\r\n",
        "1. https://aka.ms/fruit-objects からトレーニング画像をダウンロードして抽出します。抽出したフォルダーには、果物の画像のコレクションが含まれています。**注:** 一時的な回避策として、トレーニングイメージにアクセスできない場合は、https://www.github.com にアクセスしてから、https://aka.ms/fruit-objects にアクセスしてください。 \r\n",
        "2. Custom Vision ポータル [https://customvision.ai](https://customvision.ai) で、物体検出プロジェクトの _Grocery Detection_ を選択していることを確認します。次に、「**画像の追加**」 を選択し、抽出したフォルダーにすべての画像をアップロードします。\r\n",
        "\r\n",
        "![「画像の追加」 をクリックしてダウンロードした画像をアップロードする](./images/fruit-upload.jpg)\r\n",
        "\r\n",
        "3. 画像をアップロードしたら、1 つ目の画像を選択して開きます。\r\n",
        "4. 自動的に検出された領域が次の画像のように表示されるまで、画像に写っている任意のオブジェクトの上にマウスを置きます。該当オブジェクトを選択し、必要に応じて、範囲のサイズを変更して、そのオブジェクトを囲みます。\r\n",
        "\r\n",
        "![オブジェクトのデフォルトの範囲](./images/object-region.jpg)\r\n",
        "\r\n",
        "または、オブジェクトをドラッグして範囲を作成することもできます。\r\n",
        "\r\n",
        "5. オブジェクトが囲まれていれば、次に示すように、適切なオブジェクト タイプ (*りんご*、*バナナ*、*みかん*など) で新しいタグを追加します。\r\n",
        "\r\n",
        "![画像内のタグ付けされたオブジェクト](./images/object-tag.jpg)\r\n",
        "\r\n",
        "6. 画像に写っている各オブジェクトを選択してタグを付け、範囲のサイズを変更し、必要に応じて新しいタグを追加します。\r\n",
        "\r\n",
        "![画像内のタグ付けされた 2 つのオブジェクト](./images/object-tags.jpg)\r\n",
        "\r\n",
        "7. 右側の **>** リンクを使用して次の画像に移動し、そこに含まれるオブジェクトにタグを付けます。画像コレクション全体で、りんご、バナナ、みかんにタグを付ける処理を続けます。\r\n",
        "\r\n",
        "8. 最後の画像のタグ付けが終了したら、「**画像詳細**」 エディタを閉じ、「**トレーニング画像**」 ページの 「**タグ**」 で 「**タグ付き**」 を選択して、タグ付けされたすべての画像を表示します。\r\n",
        "\r\n",
        "![プロジェクトのタグ付けされた画像](./images/tagged-images.jpg)\r\n",
        "\r\n",
        "## モデルのトレーニングとテスト\r\n",
        "\r\n",
        "プロジェクト内の画像にタグを付けたので、モデルをトレーニングする準備が整いました。\r\n",
        "\r\n",
        "1. Custom Vision プロジェクトで 「**トレーニング**」 をクリックして、タグ付けした画像を使用して物体検出モデルをトレーニングします。**クイック トレーニング** オプションを選択します。\r\n",
        "2. トレーニングが完了するのを待ってから (10 分ほどかかる場合があります)、*正確性*、*再現性*、*mAP* などのパフォーマンス指標を確認します。これらは分類モデルの予測精度の指標であり、すべて高い値を示しているはずです。\r\n",
        "3. ページの右上にある 「**クイック テスト**」 をクリックし、「**画像の URL**」 ボックスに`https://aka.ms/apple-orange` と入力して、生成された予測を表示します。「**クイック テスト**」 ウィンドウを閉じます。\r\n",
        "\r\n",
        "## 物体検出モデルを発行して利用する\r\n",
        "\r\n",
        "トレーニングしたモデルを発行してクライアント アプリケーションから使用する準備が整いました。\r\n",
        "\r\n",
        "1. 「**パフォーマンス**」 ページの左上にある 「**&#128504; 発行**」 をクリックして、次の設定でトレーニングしたモデルを発行します。\r\n",
        "    - **モデル名**: 農産物検出\r\n",
        "    - **予測リソース**: *Custom Vision の**予測**リソース*\r\n",
        "\r\n",
        "### (!)確認 \r\n",
        "同じモデル名「**農産物検出**」を使用しましたか? \r\n",
        "\r\n",
        "2. 発行したら 「**パフォーマンス**」 ページの右上にある 「*設定*」 (&#9881;) アイコンをクリックして、プロジェクトの設定を表示します。左側の 「**全般**」 で、**プロジェクト ID** をコピーします。下にスクロールして、以下のステップ 5 のコード セルに貼り付けます (**YOUR_PROJECT_ID** と置き換える)。 \r\n",
        "\r\n",
        "> (*この演習の最初に **Custom Vision** リソースを作成する代わりに **Cognitive Services** リソースを使用した場合は、プロジェクト設定の右側からキーとエンドポイントをコピーして、下のコード セルに貼り付けて実行すると、結果を確認できます。それ以外の場合は、以下の手順を続行して、Custom Vision 予測リソースのキーとエンドポイントを取得してください*。)\r\n",
        "\r\n",
        "3. 「**プロジェクト設定**」 ページの左上にある 「*プロジェクト ギャラリー*」 (&#128065;) アイコンをクリックして、プロジェクトが一覧表示されている Custom Vision ポータルのホームページに戻ります。\r\n",
        "\r\n",
        "4. Custom Vision ポータルのホームページの右上にある 「*設定*」 (&#9881;) アイコンをクリックして、Custom Vision サービスの設定を表示します。「**リソース**」 で*予測*リソース (トレーニング リソース<u>ではありません</u>) を展開し、その 「**キー**」 と 「**エンドポイント**」 の値をコピーして、以下の手順 5 のコード セルに貼り付けます (**YOUR_KEY**、**YOUR_ENDPOINT** とそれぞれ置き換える)。\r\n",
        "\r\n",
        "### (!)確認 \r\n",
        "**Custom Vision** リソースを使用している場合、**予測**リソースを使用しましたか (トレーニング リソースでは<u>ありません</u>)?\r\n",
        "\r\n",
        "5. セルの左上にある 「セルの実行」 <span>&#9655;</span> ボタンをクリックして次のコード セルを実行し、変数にプロジェクト ID、キー、およびエンドポイント値を設定します。"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "project_id = 'YOUR_PROJECT_ID' # Replace with your project ID\r\n",
        "cv_key = 'YOUR_KEY' # Replace with your prediction resource primary key\r\n",
        "cv_endpoint = 'YOUR_ENDPOINT' # Replace with your prediction resource endpoint\r\n",
        "\r\n",
        "model_name = 'detect-produce' # this must match the model name you set when publishing your model iteration exactly (including case)!\r\n",
        "print('Ready to predict using model {} in project {}'.format(model_name, project_id))"
      ],
      "outputs": [],
      "metadata": {
        "gather": {
          "logged": 1599692485387
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "これで、キーとエンドポイントを Custom Vision クライアントで使用して、Custom Vision 物体検出モデルに接続できます。\r\n",
        "\r\n",
        "以下のコード セルを実行します。このコード セルは、検出モデルを使用して画像の中にある個々の農産物アイテムを検出します。\r\n",
        "\r\n",
        "> **注**: コードの詳細についてはあまり気にしないでください。Computer Vision サービス用の Python SDK を使用して、モデルに画像を送信し、検出した物体の予測を取得します。各予測は、クラス名 (*りんご*、*バナナ*、*みかん*) と、予測した物体が画像の中のどこで検出されたかを示す*境界ボックス*の座標で構成されます。この情報を使用して、画像上の各オブジェクトの周囲にラベル付きのボックスを描画します。"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "from azure.cognitiveservices.vision.customvision.prediction import CustomVisionPredictionClient\r\n",
        "from msrest.authentication import ApiKeyCredentials\r\n",
        "from matplotlib import pyplot as plt\r\n",
        "from PIL import Image, ImageDraw, ImageFont\r\n",
        "import numpy as np\r\n",
        "import os\r\n",
        "%matplotlib inline\r\n",
        "\r\n",
        "# Load a test image and get its dimensions\r\n",
        "test_img_file = os.path.join('data', 'object-detection', 'produce.jpg')\r\n",
        "test_img = Image.open(test_img_file)\r\n",
        "test_img_h, test_img_w, test_img_ch = np.array(test_img).shape\r\n",
        "\r\n",
        "# Get a prediction client for the object detection model\r\n",
        "credentials = ApiKeyCredentials(in_headers={\"Prediction-key\": cv_key})\r\n",
        "predictor = CustomVisionPredictionClient(endpoint=cv_endpoint, credentials=credentials)\r\n",
        "\r\n",
        "print('Detecting objects in {} using model {} in project {}...'.format(test_img_file, model_name, project_id))\r\n",
        "\r\n",
        "# Detect objects in the test image\r\n",
        "with open(test_img_file, mode=\"rb\") as test_data:\r\n",
        "    results = predictor.detect_image(project_id, model_name, test_data)\r\n",
        "\r\n",
        "# Create a figure to display the results\r\n",
        "fig = plt.figure(figsize=(8, 8))\r\n",
        "plt.axis('off')\r\n",
        "\r\n",
        "# Display the image with boxes around each detected object\r\n",
        "draw = ImageDraw.Draw(test_img)\r\n",
        "lineWidth = int(np.array(test_img).shape[1]/100)\r\n",
        "object_colors = {\r\n",
        "    \"apple\": \"lightgreen\",\r\n",
        "    \"banana\": \"yellow\",\r\n",
        "    \"orange\": \"orange\"\r\n",
        "}\r\n",
        "for prediction in results.predictions:\r\n",
        "    color = 'white' # default for 'other' object tags\r\n",
        "    if (prediction.probability*100) > 50:\r\n",
        "        if prediction.tag_name in object_colors:\r\n",
        "            color = object_colors[prediction.tag_name]\r\n",
        "        left = prediction.bounding_box.left * test_img_w \r\n",
        "        top = prediction.bounding_box.top * test_img_h \r\n",
        "        height = prediction.bounding_box.height * test_img_h\r\n",
        "        width =  prediction.bounding_box.width * test_img_w\r\n",
        "        points = ((left,top), (left+width,top), (left+width,top+height), (left,top+height),(left,top))\r\n",
        "        draw.line(points, fill=color, width=lineWidth)\r\n",
        "        plt.annotate(prediction.tag_name + \": {0:.2f}%\".format(prediction.probability * 100),(left,top), backgroundcolor=color)\r\n",
        "plt.imshow(test_img)\r\n"
      ],
      "outputs": [],
      "metadata": {
        "gather": {
          "logged": 1599692585672
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "検出したオブジェクトと各予測の確率を示す、予測結果を表示します。"
      ],
      "metadata": {}
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python",
      "version": "3.6.9",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kernelspec": {
      "name": "python3-azureml",
      "language": "python",
      "display_name": "Python 3.6 - AzureML"
    },
    "kernel_info": {
      "name": "python3-azureml"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}