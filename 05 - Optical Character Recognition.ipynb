{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 光学式文字認識\r\n",
        "\r\n",
        "![新聞を読んでいるロボット](./images/ocr.jpg)\r\n",
        "\r\n",
        "画像の中からテキストを検出して解釈することは、Computer Vision における一般的なタスクです。この種の処理は、通常、*光学式文字認識* (OCR) と呼ばれます。\r\n",
        "\r\n",
        "## Computer Vision サービスを使用して画像の中にあるテキストを見つける\r\n",
        "\r\n",
        "**Computer Vision** Cognitive Service は、次のような OCR タスクをサポートしています。\r\n",
        "\r\n",
        "- 複数の言語のテキストを読み取れる **OCR** API。この API は同期的に使用でき、画像の中から少量のテキストを検出して読み取る必要がある場合にうまく機能します。\r\n",
        "- サイズの大きいドキュメント用に最適化された **Read** API。この API は非同期で使用し、印刷されたテキストと手書きのテキストの両方を読み取れます。\r\n",
        "\r\n",
        "このサービスを使用するには、**Computer Vision** リソースまたは **Cognitive Services** リソースのいずれかを作成します。\r\n",
        "\r\n",
        "まだ作成していない場合は、Azure サブスクリプションに **Cognitive Services** リソースを作成します。\r\n",
        "\r\n",
        "> **注**: Cognitive Services リソースが既にある場合は、Azure portal で**クイック スタート**ページを開き、キーとエンドポイントを以下のセルにコピーするだけで作成できます。それ以外の場合は、以下の手順に従って作成してください。\r\n",
        "\r\n",
        "1. ブラウザーの新しいタブで Azure portal (https://portal.azure.com) を開き、Microsoft アカウントでサインインします。\r\n",
        "\r\n",
        "2. 「**&#65291;リソースの作成**」 ボタンをクリックし、*Cognitive Services* を検索して、以下の設定で **Cognitive Services** リソースを作成します。\r\n",
        "    - **サブスクリプション**: *使用する Azure サブスクリプション*\r\n",
        "    - **リソース グループ**: *一意の名前のリソース グループを選択または作成します*\r\n",
        "    - **リージョン**: *利用可能な任意のリージョンを選択します*。\r\n",
        "    - **名前**: *一意の名前を入力します*。\r\n",
        "    - **価格レベル**: S0\r\n",
        "    - **注意事項を読み理解しました**: 選択されています。\r\n",
        "3. デプロイが完了するまで待ちます。そのあと Cognitive Services リソースに移動し、「**概要**」 ページでリンクをクリックしてサービスのキーを管理します。クライアント アプリケーションから Cognitive Services リソースに接続するには、エンドポイントとキーが必要です。\r\n",
        "\r\n",
        "### Cognitive Services リソースのキーとエンドポイントを取得する\r\n",
        "\r\n",
        "Cognitive Services リソースを使用するには、クライアント アプリケーションにエンドポイントと認証キーが必要です。\r\n",
        "\r\n",
        "1. Azure portalで、Cognitive Services リソースの 「**キーとエンドポイント**」 ページからリソースの 「**キー 1**」 の値をコピーし、以下のコードに貼り付けます (**YOUR_COG_KEY** と置き換える)。\r\n",
        "2. リソースの**エンドポイント**をコピーして以下のコードに貼り付けます (**YOUR_COG_ENDPOINT** と置き換える)。\r\n",
        "3. セルの左側にある 「**セルの実行**」  (&#9655;) ボタンをクリックして、以下のセルのコードを実行します。"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "cog_key = 'YOUR_COG_KEY'\n",
        "cog_endpoint = 'YOUR_COG_ENDPOINT'\n",
        "\n",
        "print('Ready to use cognitive services at {} using key {}'.format(cog_endpoint, cog_key))"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1599694246277
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "キーとエンドポイントを設定したので、Computer Vision サービスのリソースを使用して画像からテキストを抽出できるようになりました。\r\n",
        "\r\n",
        "**OCR** API から始めます。これにより、画像を同期的に分析し、画像に含まれるテキストを読み取ることができます。この例では、架空の小売店 Northwind Traders の広告画像にテキストが含まれています。以下のセルを実行してテキストを読み取ります。 "
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from azure.cognitiveservices.vision.computervision import ComputerVisionClient\n",
        "from msrest.authentication import CognitiveServicesCredentials\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image, ImageDraw\n",
        "import os\n",
        "%matplotlib inline\n",
        "\n",
        "# Get a client for the computer vision service\r\n",
        "computervision_client = ComputerVisionClient(cog_endpoint, CognitiveServicesCredentials(cog_key))\n",
        "\n",
        "# Read the image file\r\n",
        "image_path = os.path.join('data', 'ocr', 'advert.jpg')\n",
        "image_stream = open(image_path, \"rb\")\n",
        "\n",
        "# Use the Computer Vision service to find text in the image\r\n",
        "read_results = computervision_client.recognize_printed_text_in_stream(image_stream)\n",
        "\n",
        "# Process the text line by line\r\n",
        "for region in read_results.regions:\n",
        "    for line in region.lines:\n",
        "\n",
        "        # Read the words in the line of text\n",
        "        line_text = ''\n",
        "        for word in line.words:\n",
        "            line_text += word.text + ' '\n",
        "        print(line_text.rstrip())\n",
        "\n",
        "# Open image to display it.\r\n",
        "fig = plt.figure(figsize=(7, 7))\n",
        "img = Image.open(image_path)\n",
        "draw = ImageDraw.Draw(img)\n",
        "plt.axis('off')\n",
        "plt.imshow(img)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1599694257280
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "画像の中から見つかったテキストは範囲、行、単語の階層構造で整理され、コードはこれらを読み取って結果を取得します。\r\n",
        "\r\n",
        "その結果、画像の中から読み取ったテキストを表示します。 \r\n",
        "\r\n",
        "## 境界ボックスを表示する\r\n",
        "\r\n",
        "結果には、画像の中にあるテキスト行と個々の単語の*境界ボックス*座標も含まれます。以下のセルを実行して、上記で取得した広告画像のテキスト行の境界ボックスを確認します。"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Open image to display it.\r\n",
        "fig = plt.figure(figsize=(7, 7))\n",
        "img = Image.open(image_path)\n",
        "draw = ImageDraw.Draw(img)\n",
        "\n",
        "# Process the text line by line\r\n",
        "for region in read_results.regions:\n",
        "    for line in region.lines:\n",
        "\n",
        "        # Show the position of the line of text\n",
        "        l,t,w,h = list(map(int, line.bounding_box.split(',')))\n",
        "        draw.rectangle(((l,t), (l+w, t+h)), outline='magenta', width=5)\n",
        "\n",
        "        # Read the words in the line of text\n",
        "        line_text = ''\n",
        "        for word in line.words:\n",
        "            line_text += word.text + ' '\n",
        "        print(line_text.rstrip())\n",
        "\n",
        "# Show the image with the text locations highlighted\r\n",
        "plt.axis('off')\n",
        "plt.imshow(img)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1599694266106
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "その結果、テキストの各行の境界ボックスが画像上に長方形として表示されます。\r\n",
        "\r\n",
        "## Read API を使用する\r\n",
        "\r\n",
        "前に使用した OCR API は、テキストが少ない画像に適しています。スキャンした文書など、より多くのテキストを読み取る必要がある場合は、**Read** API を使用します。次の各手順に従います。\r\n",
        "\r\n",
        "1. 画像を Computer Vision サービスに送信して、非同期で読み取って分析します。\r\n",
        "2. 分析処理が完了するまで待ちます。\r\n",
        "3. 分析の結果を取得します。\r\n",
        "\r\n",
        "次のセルを実行してこのプロセスを実施し、小売店 Northwind Traders のマネージャー宛ての手紙をスキャンした画像からテキストを読み取ります。"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from azure.cognitiveservices.vision.computervision import ComputerVisionClient\n",
        "from azure.cognitiveservices.vision.computervision.models import OperationStatusCodes\n",
        "from msrest.authentication import CognitiveServicesCredentials\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "import time\n",
        "import os\n",
        "%matplotlib inline\n",
        "\n",
        "# Read the image file\r\n",
        "image_path = os.path.join('data', 'ocr', 'letter.jpg')\n",
        "image_stream = open(image_path, \"rb\")\n",
        "\n",
        "# Get a client for the computer vision service\r\n",
        "computervision_client = ComputerVisionClient(cog_endpoint, CognitiveServicesCredentials(cog_key))\n",
        "\n",
        "# Submit a request to read printed text in the image and get the operation ID\r\n",
        "read_operation = computervision_client.read_in_stream(image_stream,\n",
        "                                                      raw=True)\n",
        "operation_location = read_operation.headers[\"Operation-Location\"]\n",
        "operation_id = operation_location.split(\"/\")[-1]\n",
        "\n",
        "# Wait for the asynchronous operation to complete\r\n",
        "while True:\n",
        "    read_results = computervision_client.get_read_result(operation_id)\n",
        "    if read_results.status not in [OperationStatusCodes.running]:\n",
        "        break\n",
        "    time.sleep(1)\n",
        "\n",
        "# If the operation was successfuly, process the text line by line\r\n",
        "if read_results.status == OperationStatusCodes.succeeded:\n",
        "    for result in read_results.analyze_result.read_results:\n",
        "        for line in result.lines:\n",
        "            print(line.text)\n",
        "\n",
        "# Open image and display it.\r\n",
        "print('\\n')\n",
        "fig = plt.figure(figsize=(12,12))\n",
        "img = Image.open(image_path)\n",
        "plt.axis('off')\n",
        "plt.imshow(img)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1599694312346
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "結果を確認します。手紙の大部分は印刷されたテキストで、手書きの署名もありますが、完全に読み取られています。手紙の元の画像は OCR の結果の下に表示されています。表示するにはスクロールする必要があるかもしれません。\r\n",
        "\r\n",
        "## 手書きのテキストを読み取る\r\n",
        "\r\n",
        "前の例の画像分析リクエストでは、*印刷された*テキストの処理を最適化する、テキスト認識モードを指定していました。それにもかかわらず、手書きの署名も読み取っています。\r\n",
        "\r\n",
        "手書きのテキストを読み取れるこの機能は非常に便利です。たとえば、買い物リストをメモに書き、スマートフォンのアプリを使用してメモを読み取り、メモの中からテキストを書き起こしてみます。\r\n",
        "\r\n",
        "以下のセルを実行して、手書きの買い物リストの読み取り処理の例を確認します。"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from azure.cognitiveservices.vision.computervision import ComputerVisionClient\n",
        "from azure.cognitiveservices.vision.computervision.models import OperationStatusCodes\n",
        "from msrest.authentication import CognitiveServicesCredentials\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "import time\n",
        "import os\n",
        "%matplotlib inline\n",
        "\n",
        "# Read the image file\r\n",
        "image_path = os.path.join('data', 'ocr', 'note.jpg')\n",
        "image_stream = open(image_path, \"rb\")\n",
        "\n",
        "# Get a client for the computer vision service\r\n",
        "computervision_client = ComputerVisionClient(cog_endpoint, CognitiveServicesCredentials(cog_key))\n",
        "\n",
        "# Submit a request to read printed text in the image and get the operation ID\r\n",
        "read_operation = computervision_client.read_in_stream(image_stream,\n",
        "                                                      raw=True)\n",
        "operation_location = read_operation.headers[\"Operation-Location\"]\n",
        "operation_id = operation_location.split(\"/\")[-1]\n",
        "\n",
        "# Wait for the asynchronous operation to complete\r\n",
        "while True:\n",
        "    read_results = computervision_client.get_read_result(operation_id)\n",
        "    if read_results.status not in [OperationStatusCodes.running]:\n",
        "        break\n",
        "    time.sleep(1)\n",
        "\n",
        "# If the operation was successfuly, process the text line by line\r\n",
        "if read_results.status == OperationStatusCodes.succeeded:\n",
        "    for result in read_results.analyze_result.read_results:\n",
        "        for line in result.lines:\n",
        "            print(line.text)\n",
        "\n",
        "# Open image and display it.\r\n",
        "print('\\n')\n",
        "fig = plt.figure(figsize=(12,12))\n",
        "img = Image.open(image_path)\n",
        "plt.axis('off')\n",
        "plt.imshow(img)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1599694340593
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 詳細情報\r\n",
        "\r\n",
        "OCR 向け Computer Vision Services の詳細については、「[Computer Vision ドキュメント](https://docs.microsoft.com/ja-jp/azure/cognitive-services/computer-vision/concept-recognizing-text)」を参照してください。"
      ],
      "metadata": {}
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python",
      "version": "3.6.9",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kernelspec": {
      "name": "python3-azureml",
      "language": "python",
      "display_name": "Python 3.6 - AzureML"
    },
    "kernel_info": {
      "name": "python3-azureml"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}