{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Language Understanding\n",
        "\n",
        "自然言語で話された指示や手入力された指示を理解できる、AI を活用したコンピューターへの期待が高まっています。たとえば、ホーム オートメーション システムを導入して、「明かりをつけて」や「空調をつけて」などの指示を音声で行い、自宅のデバイスを制御できるようにしたい場合があります。AI 搭載デバイスに指示を理解させて、適切なアクションを実行させます。\n",
        "\n",
        "![話を聞いているロボット](./images/language_understanding.jpg)\n",
        "\n",
        "## オーサリング リソースおよび予測リソースを作成する\n",
        "\n",
        "Microsoft Cognitive Services に含まれている Language Understanding サービスを利用すると、*発話*に基づいて*エンティティ*に適用する、*意図*を定義できます。 \n",
        "\n",
        "Language Understanding サービスを利用するには、次の 2 種類のリソースが必要です。\n",
        "\n",
        "- *オーサリング* リソース。言語モデルの定義、トレーニング、テストを行うために使用します。Azure サブスクリプションの **Language Understanding - オーサリング** リソースである必要があります。\n",
        "- *予測*リソース。モデルを発行して、モデルを使用するクライアント アプリケーションからの要求を処理するために使用します。Azure サブスクリプションの **Language Understanding** リソースまたは **Cognitive Services** リソースのどちらでも構いません。\n",
        "\n",
        "**Language Understanding** リソースまたは **Cognitive Services** リソースのいずれかを使用して Language Understanding アプリを*発行*できますが、アプリを*オーサリング*するためには別の **Language Understanding** リソースを作成する必要があります。\n",
        "\n",
        "> **重要**: オーサリング リソースは、3 つの*リージョン* (ヨーロッパ、オーストラリア、米国) のいずれかで作成する必要があります。ヨーロッパまたはオーストラリアのオーサリング リソースで作成したモデルは、それぞれヨーロッパまたはオーストラリアの予測リソースにのみ展開できます。米国のオーサリング リソースで作成したモデルは、ヨーロッパとオーストラリア以外の場所の Azure にある予測リソースにデプロイできます。オーサリングと予測の場所の詳細については、「[オーサリング リージョンと発行リージョンについてのドキュメント](https://docs.microsoft.com/azure/cognitive-services/luis/luis-reference-regions)」を参照してください。\n",
        "\n",
        "1. ブラウザーの新しいタブで、Azure portal ([https://portal.azure.com](https://portal.azure.com)) を開き、Microsoft アカウントでサインインします。\n",
        "2. 「**+ リソースの作成**」 をクリックし、*Language Understanding* を検索します。\n",
        "3. サービスの一覧から 「**Language Understanding**」 を選択します。\n",
        "4. 「**Language Understanding**」 ブレードで、「**作成**」 をクリックします。\n",
        "5. 「**作成**」 ブレードで次の詳細を入力し、「**作成**」 をクリックします。\n",
        "   - **オプションを作成**: 両方\n",
        "   - **名前**: *サービスの一意の名前*\n",
        "   - **サブスクリプション**: *Azure サブスクリプションを選択します*\n",
        "   - **リソース グループ**: *既存のリソース グループを選択するか、新しいリソース グループを作成します*\n",
        "   - **オーサリングの場所**: *任意の場所を選択します*\n",
        "   - **オーサリングの価格レベル:**: F0\n",
        "   - **予測場所**: *オーサリング場所と同じリージョンの場所を選択します*\n",
        "   - **予測の価格レベル**: F0\n",
        "   \n",
        "6. リソースが作成されるのを待ち、2 つの Language Understanding リソースがプロビジョニングされたことを確認してください。1 つはオーサリング用で、もう 1 つは予測用です。リソースを作成したリソース グループに移動すると、これらのリソースを表示できます。\n",
        "\n",
        "### Language Understanding アプリを作成する\n",
        "\n",
        "Language Understanding を使用して自然言語理解を実装するには、アプリを作成し、エンティティ、意図、発話を追加して、アプリに理解させたいコマンドを定義します。\n",
        "\n",
        "1. ブラウザーの新しいタブで、オーサリング リソースを作成したオーサリング リージョンの Language Understanding ポータルを開きます。\n",
        "    - 米国: [https://www.luis.ai](https://www.luis.ai)\n",
        "    - ヨーロッパ: [https://eu.luis.ai](https://eu.luis.ai)\n",
        "    - オーストラリア: [https://au.luis.ai](https://au.luis.ai)\n",
        "\n",
        "2. Azure サブスクリプションに関連付けられている Microsoft アカウントを使用してサインインします。Language Understanding ポータルに初めてサインインする場合は、アカウントの詳細にアクセスするためのアクセス許可をアプリに付与する必要がある場合があります。次に、Azure サブスクリプションで作成した既存の Language Understanding オーサリング リソースを選択して、「*ようこそ*」の手順を完了します。 \n",
        "\n",
        "3. 「**Conversation Apps (会話アプリ)**」 ページを開き、サブスクリプションと Language Understanding オーサリング リソースを選択します。次の設定を使用して、会話用の新しいアプリを作成します。\n",
        "   - **名前**: Home Automation\n",
        "   - **カルチャー**: 日本語 (*このオプションがない場合は空白のままにします*)\n",
        "   - **説明**: 単純なホーム オートメーション\n",
        "   - **予測リソース**: *利用する Language Understanding 予測リソース*\n",
        "\n",
        "4. 効果的な Language Understanding アプリを作成するためのヒントが記載されたパネルが表示された場合は、それを閉じます。\n",
        "\n",
        "### エンティティを作成する\n",
        "\n",
        "*エンティティ*とは、言語モデルが識別でき、取り扱える事がらです。ここでは、Language Understanding アプリを使用して、電気や空調などのオフィス内のさまざまな*デバイス*を制御します。そのため、アプリで扱う種類のデバイスのリストを含む、*デバイス* エンティティを作成します。デバイス タイプごとに、デバイスの名前 (*明かり*など) と、このタイプのデバイスの参照に使う可能性のある同義語 (*ライト*など) を識別するサブリストを作成します。\n",
        "\n",
        "1. アプリの 「Language Understanding」 ページの左側のペインで、「**Entities (エンティティ)**」 をクリックします。次に 「**作成**」 をクリックして「**device**」という名前の新しいエンティティを作成し、「**List (リスト)**」 タイプを選択して 「**作成**」 をクリックします。\n",
        "2. 「**List items (アイテムの一覧)**」 ページの 「**Normalized Values (正規化された値)**」 に「**明かり**」と入力し、Enter キーを押します。\n",
        "3. 「**明かり**」値を追加したら、「**Synonyms (同義語)**」 に「**ライト**」と「**light**」と入力し、Enter キーを押します。\n",
        "4. 「**空調**」という名前の 2 つ目のリスト アイテムを追加し、同義語を「**エアコン**」にします。\n",
        "\n",
        "> **注**: このラボでは、指示どおりに正確なひらがなや漢字を使用し _(「 あかり」**ではなく**「明かり」など)_、余分なスペースを追加しないでください。 \n",
        "\n",
        "### 意図を作成する\n",
        "\n",
        "*意図*とは、エンティティに対して実行するアクションです。たとえば、明かりをつけたり、空調を消すことです。ここでは、2 つの意図を定義します。1 つはデバイスをオンにするためのもので、もう 1 つはデバイスをオフにするためのものです。それぞれの意図に*発話*のサンプルを指定し、意図を表すために使用する言語の種類を示します。\n",
        "\n",
        "> **注**: このラボでは、指示どおりに正確なひらがなや漢字を使用し、余分なスペースを追加しないでください _(「あかりをつけて 。」ではなく「明かりをつけて」と入力)_、 \n",
        "\n",
        "1. 左側のペインで 「**Intents (意図)**」 をクリックします。「**作成**」 をクリックして、意図に「**switch_on**」という名前を追加して、「**完了**」 をクリックします。\n",
        "2. 「**Examples (例)**」 見出しの下の、「**Example user input (ユーザー入力の例)**」 サブ見出しで、発話「***明かりをつけて***」を入力して **Enter** キーを押し、この発話をリストに送信します。さらに「**Turn the light on**」を入力し **Enter** キーを押して追加します(音声コントロールパート用)。\n",
        "3. 発話「*明かりをつけて*」で「明かり」という単語をクリックし、「**device**」エンティティの「**明かり**」値に割り当てます。\n",
        "\n",
        "![「明かり」という単語をエンティティ値に割り当てる方法](./images/assign_entity.jpg)\n",
        "\n",
        "4. 意図「**switch_on**」に「***空調をオンにして***」というフレーズで 2 つ目の発話を追加します。「空調」という単語を「**device**」エンティティの「**空調**」値に割り当てます。\n",
        "5. 左側のペインで 「**Intents (意図)**」 をクリックして 「**作成**」 をクリックし、「**switch_off**」という名前の 2 つ目の意図を追加します。\n",
        "6. 意図「**switch_off**」の 「**Utterances (発話)**」 ページで発話「***明かりを消して***」を追加して、「**device**」エンティティの「**明かり**」値に「明かり」という単語を割り当てます。\n",
        "7. 意図「**switch_off**」に「***空調をオフにして***」というフレーズで 2 つ目の発話を追加します。「空調」という単語を「**device**」エンティティの「**空調**」値に関連付けます。\n",
        "\n",
        "### 言語モデルのトレーニングとテスト\n",
        "\n",
        "これで、エンティティ、意図、発話の形式で与えたデータを使用して、アプリの言語モデルをトレーニングする準備が整いました。\n",
        "\n",
        "1. アプリの 「Language Understanding」 ページの上部にある 「**トレーニング**」 をクリックして、言語モデルをトレーニングします。\n",
        "2. モデルがトレーニングされたら 「**Test (テスト)**」 をクリックし、「Test (テスト)」 ペインで次のフレーズに対して予測される意図を表示します。\n",
        "    * *明かりをオンにして*\n",
        "    * *空調消して*\n",
        "    * *ライトを消して*\n",
        "    * *エアコンをつけて*\n",
        "3. テスト ウィンドウを閉じます。\n",
        "    \n",
        "### モデルを発行し、エンドポイントを構成する\n",
        "\n",
        "トレーニングしたモデルをクライアント アプリケーションで使用するには、モデルをエンドポイントとして発行する必要があります。このエンドポイントに対してクライアント アプリケーションは新しい発話を送信し、このエンドポイントから意図とエンティティが予測されます。\n",
        "\n",
        "1. アプリの 「Language Understanding」 ページの上部にある 「**Publish (発行)**」 をクリックします。「**運用スロット**」 を選択し 「**完了**」 をクリックします。\n",
        "\n",
        "2. モデルが発行されたら、アプリの「Language Understanding」ページの上部にある 「**管理**」 をクリックします。次に 「**設定**」 タブにあるアプリの 「**アプリ ID**」 をメモします。この ID をコピーして以下のコードに貼り付けます (**YOUR_LU_APP_ID** と置き換える)。\n",
        "\n",
        "3. 「**Azure リソース**」 タブにある予測リソースの 「**主キー**」 と 「**エンドポイント URL**」 の値をメモします。これらの値をコピーして以下のコードに貼り付けます (**YOUR_LU_KEY**、**YOUR_LU_ENDPOINT** とそれぞれ置き換える)。\n",
        "\n",
        "4. セルの左側にある 「**セルの実行**」 (&#9655;) ボタンをクリックして以下のセルを実行し、プロンプトが表示されたらテキスト「*明かりをつけて*」と入力します。Language Understanding モデルがテキストを解釈し、適切な画像を表示します。\n",
        "\n",
        "### **(!) 重要**: \n",
        "ウィンドウの上部にあるプロンプトを探します。「*明かりをつけて*」と入力して **Enter** キーを押す必要があります。 \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1599696381331
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "from python_code import luis\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "import os\n",
        "%matplotlib inline\n",
        "\n",
        "try:\n",
        "    # Set up API configuration\n",
        "    luis_app_id = 'YOUR_LU_APP_ID'\n",
        "    luis_key = 'YOUR_LU_KEY'\n",
        "    luis_endpoint = 'YOUR_LU_ENDPOINT'\n",
        "\n",
        "    # prompt for a command\n",
        "    command = input('Please enter a command: \\n')\n",
        "\n",
        "     # get the predicted intent and entity (code in python_code.home_auto.py)\n",
        "    action = luis.get_intent(luis_app_id, luis_key, luis_endpoint, command)\n",
        "\n",
        "     # display an appropriate image by English name\n",
        "    img_name = action.replace('明かり', 'light').replace('空調', 'fan') + '.jpg'\n",
        "    img = Image.open(os.path.join(\"data\", \"luis\" ,img_name))\n",
        "    plt.axis('off')\n",
        "    plt. imshow(img)\n",
        "except Exception as ex:\n",
        "    print(ex)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### (!)確認 \n",
        "上記のセルを実行してプロンプトが表示され、フレーズ「*明かりをつけて*」を入力しましたか? プロンプトはウィンドウの上部に表示されます。  \n",
        "\n",
        "上のセルを再度実行して、次のフレーズを試します。\n",
        "\n",
        "* *明かりつけて*\n",
        "* *ライトを消して*\n",
        "* *空調をオンにして*\n",
        "* *明かりをオンにして*\n",
        "* *明かりをオフにして*\n",
        "* *空調消して*\n",
        "* *エアコンをつけて*\n",
        "\n",
        "上のセルを実行して疑問符の画像が表示された場合は、エンティティ、意図、発話などを作成したときに、指示されたものとはわずかに異なるテキストを使用したり、空白を挿入した可能性があります。\n",
        "\n",
        "> **注**: Language Understanding アプリから意図とエンティティを取得するコードに関心がある場合は、**python_code** フォルダーにある **luis.py** ファイルを参照してください。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 音声コントロールを追加する\n",
        "\n",
        "これまで、テキストを分析する方法を見てきました。いっぽう、音声を認識できる AI システムによって、人間はソフトウェア サービスとコミュニケーションできるようになってきました。**Speech** Cognitive Service を使用すると、簡単に話し言葉をテキストに変換できます。\n",
        "\n",
        "### Cognitive Services リソースを作成する\n",
        "\n",
        "まだリソースを作成していない場合は、次の手順で Azure サブスクリプションに **Cognitive Services** リソースを作成します。\n",
        "\n",
        "> **注**: Cognitive Services リソースが既にある場合は、Azure portal で 「**クイック スタート**」 ページを開き、キーと場所を以下のセルにコピーするだけで作成できます。それ以外の場合は、以下の手順に従って作成してください。\n",
        "\n",
        "1. ブラウザーの新しいタブで、Azure portal ([https://portal.azure.com](https://portal.azure.com)) を開き、Microsoft アカウントでサインインします。\n",
        "2. 「**&#65291;リソースの作成**」 ボタンをクリックし、*Cognitive Services* を検索して、以下の設定で **Cognitive Services** リソースを作成します。\n",
        "    - **サブスクリプション**: *使用する Azure サブスクリプション*\n",
        "    - **リソース グループ**: *一意の名前のリソース グループを選択または作成します*\n",
        "    - **リージョン**: *利用可能な任意のリージョンを選択します*。\n",
        "    - **名前**: *一意の名前を入力します*。\n",
        "    - **価格レベル**: S0\n",
        "    - **このチェック ボックスをオンにすることにより、このサービスの使用が米国の警察によるものではないことを証明します。**: 選択されています。\n",
        "    - **注意事項を読み理解しました**: 選択されています。\n",
        "3. デプロイが完了するまで待ちます。そのあと Cognitive Services リソースに移動し、「**クイック スタート**」 ページにあるキーと場所をメモします。クライアント アプリケーションから Cognitive Services リソースに接続するにはキーと場所が必要です。\n",
        "\n",
        "### Cognitive Services リソースのキーと場所を取得する\n",
        "\n",
        "Cognitive Services リソースを使用するには、クライアント アプリケーションに認証キーと場所が必要です。\n",
        "\n",
        "1. Azure portalで、Cognitive Services リソースの 「**キーとエンドポイント**」 ページからリソースの 「**キー 1**」 の値をコピーし、以下のコードに貼り付けます (**YOUR_COG_KEY** と置き換える)。\n",
        "2. リソースの**場所**をコピーして以下のコードに貼り付けます (**YOUR_COG_LOCATION** と置き換える)。\n",
        ">**注**: 「**キーとエンドポイント**」 ページにとどまり、このページから**場所**をコピーします (例: _westus_)。「場所」 フィールドの単語の間には空白を入れ _ないで_ ください。 \n",
        "3. 以下のセルのコードを実行します。 "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1599696409914
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "cog_key = 'YOUR_COG_KEY'\n",
        "cog_location = 'YOUR_COG_LOCATION'\n",
        "\n",
        "print('Ready to use cognitive services in {} using key {}'.format(cog_location, cog_key))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "次に、以下のセルを実行して音声ファイルから音声を書き起こし、Language Understanding アプリのコマンドとして使用します。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1599696420498
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from python_code import luis\n",
        "from azure.cognitiveservices.speech import SpeechConfig, SpeechRecognizer, AudioConfig\n",
        "from playsound import playsound\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "%matplotlib inline\n",
        "\n",
        "try:   \n",
        "\n",
        "    # Get spoken command from audio file\n",
        "    file_name = 'light-on.wav'\n",
        "    audio_file = os.path.join('data', 'luis', file_name)\n",
        "\n",
        "    # Configure speech recognizer\n",
        "    speech_config = SpeechConfig(cog_key, cog_location)\n",
        "    audio_config = AudioConfig(filename=audio_file) # Use file instead of default (microphone)\n",
        "    speech_recognizer = SpeechRecognizer(speech_config, audio_config)\n",
        "\n",
        "    # Use a one-time, synchronous call to transcribe the speech\n",
        "    speech = speech_recognizer.recognize_once()\n",
        "\n",
        "    # Get the predicted intent and entity (code in python_code.home_auto.py)\n",
        "    action = luis.get_intent(luis_app_id, luis_key, luis_endpoint, speech.text)\n",
        "\n",
        "    # Get the appropriate image by English device name\n",
        "    img_name = action.replace('明かり', 'light').replace('空調', 'fan') + '.jpg'\n",
        "\n",
        "    # Display image \n",
        "    img = Image.open(os.path.join(\"data\", \"luis\" ,img_name))\n",
        "    plt.axis('off')\n",
        "    plt. imshow(img)\n",
        "    playsound(audio_file)\n",
        "\n",
        "except Exception as ex:\n",
        "    print(ex)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "上記のセルを変更して、音声ファイル **light-off.wav** を使用してみてください。\n",
        "\n",
        "## 詳細情報\n",
        "\n",
        "Language Understanding の詳細については、「[サービス ドキュメント](https://docs.microsoft.com/azure/cognitive-services/luis/)」を参照してください。"
      ]
    }
  ],
  "metadata": {
    "kernel_info": {
      "name": "python3-azureml"
    },
    "kernelspec": {
      "display_name": "Python 3.6 - AzureML",
      "language": "python",
      "name": "python3-azureml"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
